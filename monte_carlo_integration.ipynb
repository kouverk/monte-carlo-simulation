{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ² Monte Carlo Integration\n",
    "### Comparing numerical approximation vs exact values â€” with error convergence plots\n",
    "\n",
    "We'll integrate two functions:\n",
    "1. **Simple:** $f(x) = x^2$ on $[0, 3]$ â€” exact answer: $9$\n",
    "2. **Nasty:** $f(x) = \\cos(x) \\cdot e^{-x^2/2} \\cdot \\sqrt{|W(|x|+1)|}$ â€” no closed form, numerical ground truth via `scipy.integrate.quad`\n",
    "\n",
    "where $W$ is the Lambert W function (the inverse of $x e^x$).\n",
    "\n",
    "Monte Carlo integration exploits the identity:\n",
    "$$\\int_a^b f(x)\\, dx = (b-a) \\cdot \\mathbb{E}[f(X)], \\quad X \\sim \\text{Uniform}(a, b)$$\n",
    "\n",
    "As $N \\to \\infty$, the error goes as $\\sim 1/\\sqrt{N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done. Let's go.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.integrate import quad\n",
    "from scipy.special import lambertw\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "ACCENT1 = '#00d4ff'   # cyan\n",
    "ACCENT2 = '#ff6b6b'   # red-pink\n",
    "ACCENT3 = '#ffd93d'   # yellow\n",
    "ACCENT4 = '#6bcb77'   # green\n",
    "print('Imports done. Let\\'s go.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact  âˆ« xÂ² dx  over [0.0, 3.0]  =  9.0\n",
      "Quad   âˆ« nasty  over [-3.0, 3.0]  â‰ˆ  1.2690963429  (quad err â‰ˆ 1.13e-12)\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Simple function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f_simple(x):\n",
    "    \"\"\"f(x) = x^2\"\"\"\n",
    "    return x**2\n",
    "\n",
    "A_SIMPLE, B_SIMPLE = 0.0, 3.0\n",
    "EXACT_SIMPLE = 9.0  # âˆ«â‚€Â³ xÂ² dx = [xÂ³/3]â‚€Â³ = 9\n",
    "\n",
    "# â”€â”€ Nasty function (Lambert W edition) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f_nasty(x):\n",
    "    \"\"\"\n",
    "    f(x) = cos(x) * exp(-xÂ²/2) * sqrt(W(|x| + 1))\n",
    "\n",
    "    W is the Lambert W function: inverse of z = wÂ·e^w\n",
    "    This has no elementary antiderivative.\n",
    "    \"\"\"\n",
    "    W_val = np.real(lambertw(np.abs(x) + 1))   # principal branch, always real here\n",
    "    return np.cos(x) * np.exp(-x**2 / 2) * np.sqrt(W_val)\n",
    "\n",
    "A_NASTY, B_NASTY = -3.0, 3.0\n",
    "\n",
    "# Ground truth via adaptive quadrature (scipy)\n",
    "EXACT_NASTY, quad_err = quad(f_nasty, A_NASTY, B_NASTY, limit=200)\n",
    "print(f'Exact  âˆ« xÂ² dx  over [{A_SIMPLE}, {B_SIMPLE}]  =  {EXACT_SIMPLE}')\n",
    "print(f'Quad   âˆ« nasty  over [{A_NASTY}, {B_NASTY}]  â‰ˆ  {EXACT_NASTY:.10f}  (quad err â‰ˆ {quad_err:.2e})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualise the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Functions to Integrate', fontsize=15, color='white', y=1.02)\n",
    "\n",
    "# Simple\n",
    "xs = np.linspace(A_SIMPLE, B_SIMPLE, 500)\n",
    "axes[0].plot(xs, f_simple(xs), color=ACCENT1, lw=2.5)\n",
    "axes[0].fill_between(xs, f_simple(xs), alpha=0.25, color=ACCENT1)\n",
    "axes[0].set_title(r'$f(x) = x^2$  on  $[0, 3]$', color='white')\n",
    "axes[0].set_xlabel('x'); axes[0].set_ylabel('f(x)')\n",
    "axes[0].text(0.5, 0.85, f'Exact integral = {EXACT_SIMPLE}',\n",
    "             transform=axes[0].transAxes, color=ACCENT3, ha='center', fontsize=11)\n",
    "\n",
    "# Nasty\n",
    "xn = np.linspace(A_NASTY, B_NASTY, 1000)\n",
    "axes[1].plot(xn, f_nasty(xn), color=ACCENT2, lw=2.5)\n",
    "axes[1].fill_between(xn, f_nasty(xn), alpha=0.25, color=ACCENT2)\n",
    "axes[1].set_title(r'$f(x)=\\cos(x)\\,e^{-x^2/2}\\,\\sqrt{W(|x|+1)}$  on  $[-3, 3]$', color='white')\n",
    "axes[1].set_xlabel('x'); axes[1].set_ylabel('f(x)')\n",
    "axes[1].text(0.5, 0.10, f'Numerical integral â‰ˆ {EXACT_NASTY:.6f}',\n",
    "             transform=axes[1].transAxes, color=ACCENT3, ha='center', fontsize=11)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(0, color='gray', lw=0.5)\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing error curves ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_integrate(func, a, b, n_samples, seed=42):\n",
    "    \"\"\"\n",
    "    Estimate âˆ«â‚áµ‡ f(x) dx using n_samples uniform random samples.\n",
    "    Returns the estimate.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = rng.uniform(a, b, n_samples)\n",
    "    return (b - a) * np.mean(func(x))\n",
    "\n",
    "\n",
    "def mc_error_curve(func, a, b, exact, n_values, n_trials=20, seed=0):\n",
    "    \"\"\"\n",
    "    For each N in n_values, run n_trials independent MC estimates.\n",
    "    Returns:\n",
    "        mean_errors   â€” mean |estimate - exact| across trials\n",
    "        std_errors    â€” std of absolute errors (for shaded band)\n",
    "        all_estimates â€” shape (n_trials, len(n_values)) raw estimates\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_estimates = np.zeros((n_trials, len(n_values)))\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        for j, n in enumerate(n_values):\n",
    "            x = rng.uniform(a, b, n)\n",
    "            all_estimates[t, j] = (b - a) * np.mean(func(x))\n",
    "\n",
    "    abs_errors = np.abs(all_estimates - exact)\n",
    "    return abs_errors.mean(axis=0), abs_errors.std(axis=0), all_estimates\n",
    "\n",
    "\n",
    "# Sample counts â€” log-spaced from 10 to 1,000,000\n",
    "N_VALUES = np.unique(np.logspace(1, 6, 60).astype(int))\n",
    "N_TRIALS = 30\n",
    "\n",
    "print('Computing error curves ...')\n",
    "err_simple_mean, err_simple_std, est_simple = mc_error_curve(\n",
    "    f_simple, A_SIMPLE, B_SIMPLE, EXACT_SIMPLE, N_VALUES, N_TRIALS)\n",
    "\n",
    "err_nasty_mean, err_nasty_std, est_nasty = mc_error_curve(\n",
    "    f_nasty, A_NASTY, B_NASTY, EXACT_NASTY, N_VALUES, N_TRIALS)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error vs N  â€”  The Main Event ðŸ“‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Monte Carlo Integration: Absolute Error vs Sample Count N',\n",
    "             fontsize=14, color='white', y=1.01)\n",
    "\n",
    "def plot_error_curve(ax, n_vals, mean_err, std_err, color, title, exact_label):\n",
    "    # Theoretical 1/âˆšN reference line (scaled to match)\n",
    "    scale = mean_err[5] * np.sqrt(n_vals[5])   # calibrate at a mid point\n",
    "    ref_line = scale / np.sqrt(n_vals)\n",
    "\n",
    "    # Shaded trial spread\n",
    "    ax.fill_between(n_vals,\n",
    "                    np.maximum(mean_err - std_err, 1e-10),\n",
    "                    mean_err + std_err,\n",
    "                    alpha=0.2, color=color, label='Â±1 std (across trials)')\n",
    "\n",
    "    ax.loglog(n_vals, mean_err, color=color, lw=2.5, label='Mean |error|')\n",
    "    ax.loglog(n_vals, ref_line, '--', color=ACCENT3, lw=1.5, alpha=0.8,\n",
    "              label=r'$\\sim 1/\\sqrt{N}$ reference')\n",
    "\n",
    "    ax.set_xlabel('N  (number of samples)', fontsize=12)\n",
    "    ax.set_ylabel('Absolute Error', fontsize=12)\n",
    "    ax.set_title(title, color='white', fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, which='both', alpha=0.15)\n",
    "    ax.text(0.97, 0.92, f'True value: {exact_label}',\n",
    "            transform=ax.transAxes, ha='right', color=ACCENT3, fontsize=10)\n",
    "\n",
    "plot_error_curve(axes[0], N_VALUES, err_simple_mean, err_simple_std,\n",
    "                 ACCENT1, r'$f(x)=x^2$ on $[0,3]$', f'{EXACT_SIMPLE}')\n",
    "\n",
    "plot_error_curve(axes[1], N_VALUES, err_nasty_mean, err_nasty_std,\n",
    "                 ACCENT2, r'$f(x)=\\cos(x)\\,e^{-x^2/2}\\,\\sqrt{W(|x|+1)}$ on $[-3,3]$',\n",
    "                 f'{EXACT_NASTY:.6f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Notice both curves hug the 1/âˆšN slope â€” that\\'s the Central Limit Theorem at work.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Estimate Distribution at Fixed N â€” Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a few N values and show distribution of MC estimates across trials\n",
    "HIGHLIGHT_N = [100, 1_000, 10_000, 100_000]\n",
    "N_HIST_TRIALS = 2000\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "fig.suptitle('Distribution of MC Estimates Across 2000 Trials', fontsize=13, color='white')\n",
    "\n",
    "rng = np.random.default_rng(99)\n",
    "\n",
    "for col, n in enumerate(HIGHLIGHT_N):\n",
    "    for row, (func, a, b, exact, color, fname) in enumerate([\n",
    "        (f_simple, A_SIMPLE, B_SIMPLE, EXACT_SIMPLE, ACCENT1, r'$x^2$'),\n",
    "        (f_nasty,  A_NASTY,  B_NASTY,  EXACT_NASTY,  ACCENT2, 'nasty')\n",
    "    ]):\n",
    "        samples = np.array([\n",
    "            (b - a) * np.mean(func(rng.uniform(a, b, n)))\n",
    "            for _ in range(N_HIST_TRIALS)\n",
    "        ])\n",
    "        ax = axes[row, col]\n",
    "        ax.hist(samples, bins=60, color=color, alpha=0.75, edgecolor='none')\n",
    "        ax.axvline(exact, color=ACCENT3, lw=2, linestyle='--', label=f'True = {exact:.4f}')\n",
    "        ax.axvline(samples.mean(), color='white', lw=1.5, linestyle=':', label=f'MC mean = {samples.mean():.4f}')\n",
    "        ax.set_title(f'{fname},  N={n:,}', color='white', fontsize=9)\n",
    "        ax.set_xlabel('Estimate'); ax.set_ylabel('Count' if col == 0 else '')\n",
    "        ax.legend(fontsize=7)\n",
    "        ax.grid(alpha=0.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Live Convergence Animation  ðŸŽ¬\n",
    "Watch the running estimate zero in on the true value as N grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def running_mc_estimate(func, a, b, n_max, seed=7):\n",
    "    \"\"\"Return running MC estimate at every step.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = rng.uniform(a, b, n_max)\n",
    "    fx = func(x)\n",
    "    running = (b - a) * np.cumsum(fx) / np.arange(1, n_max + 1)\n",
    "    return running\n",
    "\n",
    "N_ANIM = 5000\n",
    "run_simple = running_mc_estimate(f_simple, A_SIMPLE, B_SIMPLE, N_ANIM)\n",
    "run_nasty  = running_mc_estimate(f_nasty,  A_NASTY,  B_NASTY,  N_ANIM)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Live Convergence of MC Estimate', color='white', fontsize=13)\n",
    "\n",
    "lines, hlines = [], []\n",
    "for ax, run, exact, color, title in zip(\n",
    "    axes,\n",
    "    [run_simple, run_nasty],\n",
    "    [EXACT_SIMPLE, EXACT_NASTY],\n",
    "    [ACCENT1, ACCENT2],\n",
    "    [r'$x^2$ on $[0,3]$', r'Nasty on $[-3,3]$']\n",
    "):\n",
    "    line, = ax.plot([], [], color=color, lw=1.5)\n",
    "    ax.axhline(exact, color=ACCENT3, lw=1.5, ls='--', label=f'True = {exact:.5f}')\n",
    "    ax.set_xlim(1, N_ANIM); ax.set_ylim(exact * 0.7, exact * 1.3)\n",
    "    ax.set_xlabel('N'); ax.set_ylabel('Running Estimate')\n",
    "    ax.set_title(title, color='white'); ax.legend(fontsize=9); ax.grid(alpha=0.2)\n",
    "    lines.append(line)\n",
    "\n",
    "def update(frame):\n",
    "    idx = frame + 1\n",
    "    for line, run in zip(lines, [run_simple, run_nasty]):\n",
    "        line.set_data(np.arange(1, idx + 1), run[:idx])\n",
    "    return lines\n",
    "\n",
    "FRAMES = np.unique(np.concatenate([\n",
    "    np.arange(0, 100),\n",
    "    np.linspace(100, N_ANIM - 1, 150).astype(int)\n",
    "]))\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=FRAMES, interval=30, blit=True)\n",
    "plt.close()\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{'N':>10}  |  {'xÂ²  estimate':>14}  {'error':>12}  ||  {'nasty estimate':>16}  {'error':>12}')\n",
    "print('-' * 80)\n",
    "rng = np.random.default_rng(0)\n",
    "for n in [10, 100, 1_000, 10_000, 100_000, 1_000_000]:\n",
    "    xs = rng.uniform(A_SIMPLE, B_SIMPLE, n)\n",
    "    xn = rng.uniform(A_NASTY,  B_NASTY,  n)\n",
    "    est_s = (B_SIMPLE - A_SIMPLE) * np.mean(f_simple(xs))\n",
    "    est_n = (B_NASTY  - A_NASTY)  * np.mean(f_nasty(xn))\n",
    "    print(f'{n:>10,}  |  {est_s:>14.6f}  {abs(est_s - EXACT_SIMPLE):>12.2e}  ||  {est_n:>16.8f}  {abs(est_n - EXACT_NASTY):>12.2e}')\n",
    "\n",
    "print(f'{'Exact':>10}  |  {EXACT_SIMPLE:>14.6f}  {'â€”':>12}  ||  {EXACT_NASTY:>16.8f}  {'â€”':>12}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "- **Monte Carlo integration** converges at $O(1/\\sqrt{N})$ regardless of dimension â€” making it uniquely powerful in high dimensions.\n",
    "- The **Lambert W function** shows up in problems like `xÂ·eË£ = c` and makes analytic integration essentially impossible here.\n",
    "- Both simple and complex functions converge at the same **asymptotic rate** â€” the constant differs based on the variance of $f$ under the sampling distribution.\n",
    "- The **histograms** illustrate the CLT: as $N$ grows, the distribution of estimates tightens symmetrically around the true value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (monte-carlo-simulation)",
   "language": "python",
   "name": "monte-carlo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
